---
title: "Logboek 3.1 Metagenomics"
author: "Yamila Timmer"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: true
    toc_depth: '2'
    number_sections: true
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
lang: nl
bibliography: library.bib
---

- meer gedachtegang geven/waar ik tegen aanloop
- snakemake erin zetten
- uitleggen hoe ik dingen heb opgelost

 
# Used tools

| Package Name        | Description                                                 | Version   |
|---------------------|-------------------------------------------------------------|-----------|
| [Kraken2](https://github.com/DerrickWood/kraken2)|Taxonomic classification of 16S data |2.1.3|
| [Bracken](https://github.com/jenniferlu717/Bracken)|Re-estimating abundances from Kraken2 output|3.0.1|
| [Krona](https://github.com/marbl/Krona)|Visualising taxonomic classification in a sample|2.8.1|
| [kraken-biom](https://github.com/smdabdoub/kraken-biom)|Converting kraken file to json formatted .biom|1.2.0|
| [FAPROTAX](http://www.loucalab.com/archive/FAPROTAX/lib/php/index.php?section=Home) |Functional analysis for finding (metabolic) pathways that match the founc OTU's|1.2.10|
	

# Introduction

This introduction was written based on an overview of the purification processes and the problems that ACFC faces with, provided by Peter Matuku, a professor from the university of Eldoret, that is specialized in water purification.

For this metagenomics project we will look at a Kenyan factory, owned by [Agro Chemical & Food Company Limited or ACFC](https://acfc.co.ke/), that produces industrial spirits and yeast from sugar molasses (waste product of sugar production). Specifically we will take a closer look at microbial communities present in different steps in the water purification process of this factory. 

The production of alcohol and yeast from molasses generates a high volume of wastewater rich in organic matter, nutrients and recalcitrant compounds (slowly biodegradable or non-biodegradable compounds). The first step in the water purification process is anaerobic digestion, which facilitates organic matter degradation and also produces biogas as a byproduct. The efficiency of anaerobic treatment depends on the dynamics within the microbial community involved in the process. Looking into these dynamics using metagenomics, could offer a way to improve the overall efficiency of the digester.

The following step after anaerobic digestion, is aerobic digestion. The wastewater flows into large, but undeep lagoons where the water is exposed to a lot of oxygen. The wastewater remains in the lagoons for a number of days. After this the water flows back into the Nyando river.

The main problem is that the effluent from the lagoons does not meet Kenyan standards for water pollution, as certain qualities as the amount of organic matter and nutrients are too high. This causes damage not only for animals and plants in the river, but also to people that live downstream, that use this water for bathing, cooking and drinking.

# Our data

**05-03-2025**

Minion 16S data, taken from 3 different locations (digester, lagoon influent/lagoon effluent), with 4 replicates, with minor differences being that some samples have added glycerol, to protect the samples in the freezer. And half of the samples have been shaken. The lagoon influent (which is the effluent of the digester), will be called `lagoon in` from now. And the lagoon effluent (which is the river influent), will be called `lagoon out` from now.

|    | Sample Location | Glycerol | Shaken |
|----|-----------------|----------|--------|
| 1  | Lagoon in       | +        | -      |
| 2  | Lagoon in       | +        | +      |
| 3  | Lagoon in       | -        | -      |
| 4  | Lagoon in       | -        | +      |
| 5  | Lagoon out      | +        | -      |
| 6  | Lagoon out      | +        | +      |
| 7  | Lagoon out      | -        | -      |
| 8  | Lagoon out      | -        | +      |
| 9  | Digestor        | +        | -      |
| 10 | Digestor        | +        | +      |
| 11 | Digestor        | -        | -      |
| 12 | Digestor        | -        | +      |

We will merge the data from the samples, depending on the location of origin. So sample 1-4, sample 5-8 and sample 9-12 will be merged, as for our research question it is not important whether glycerol got added or not and whether the sample was shaken or not. Sample 10 and 12 are absent, which means that the quality of these samples was poor and that the barcodes could not be read. However this will likely not be a problem, as the other two digestor samples still remain.


**16-03-2025**
The digestor samples seem to be of very low quality, with in total resulting in 1 read (after trimming, before trimming it had 2 reads). The minion flowcell used for sequencing the data had already been used before and was likely reaching its quotum. The digestor samples got sequenced last and that is most likely the reason for the low quality of the samples. 

1 read is clearly not enough to be able to give an overview of the micro organisms present in the digestor sample, for this reason we will not be further using the digester samples and we will discuss with the client about what other research questions we could have for the lagoon in/out samples. The general research questions remain: what is present in the samples (taxonomic analysis) and what do the present micro organisms do (functional analysis). So for the pipeline we will only work with the lagoon in/out samples.


# Research question

**17-03-2025**

For this metagenomics project we have two main research questions, which are:
- What Operational Taxonomic Units (OTU's) are present in the two samples: `lagoon_in` and `lagoon_out`
- What (metabolic) processes take place in the samples of `lagoon_in` and `lagoon_out`

These two main questions are a way to get a first insight into the two samples, after we have constructed a general profile of what OTU's live in the samples and what processes happen in the samples, we would like to compare the two samples to eachother:

- What can be said on the difference/similarity of the processes that take place in the samples?
- What can be said on the diversity in OTU's, looking at the alpha diversity?

## Hypothesis
What outcome do i expect?


# Metagenomics Pipeline

**03-03-2025**

Before my group can get started on designing the pipeline for our specific case, I would like to look into what steps metagenomics consists of and what kind of tools get used. 

Steps:

- Quality control
- Trimming (if needed)
- Classification (determining what OTU's are present)
- Functional analysis (determining what processes take place in the samples)
- Calculating alpha/beta diversity
- Visualization

Floris will be looking into the quality control and trimming steps. Quality control had already been performed for us, but our teacher recommended us to repeat this step, as it is always good to double check the data from a dataset that gets handed to you.

Jarno will look into how we can calculate the alpha/beta diversity which gives an idea of the diversity of OTU's in the two samples.

I will be looking into the classification and functional analysis steps. For this I will be using Kraken2 and FAPROTAX. And together we will look into a way to visualise our results, for which I will look into Krona.

Finding a tool for functional analysis that can actually work with Kraken2 output was pretty hard. Most functional analysis tools work with outputs from other classification tools such as qiime and Humann. However I found that some people on forums mentioned that it is possible to convert kraken2 output to biom format and then plug it into a tool called faprotax.





The two tools I will be looking at are kraken 2 and krona, used for classification and visualisation respectively. Before implementing these tools into our pipeline, I will do some research on the tools.

## Base Calling

**03-03-2025**

This step needs to be done when nanopore sequencing has been used (such as with MinION). This form of sequencing results in a 'squiggle', this is a long electrical signal that has peaks and dips signifying what kind of molecule passed through the nanopore at that moment. The passing of a molecule through the pore disrupts the ionic current flowing across it and adds a characteristical electrical signal to the 'squiggle'. 

However, before you can move on to following steps in the metagenomics process, you first have to convert this electrical signal to a DNA or RNA sequence. This is normally done using **Dorado**, this is the default basecaller that is integrated within MinKNOW (the software used to control sequencing devices such as MinION) @OxfordNanopore_Basecalling.

Dorado works optimally with the POD5 file-format. Our data consists of fast5 files, so we will have to convert these first.

**04-03-2025**
Our teacher has mentioned that base calling had already been performed on the data and that we were not expected to re-do this, so instead we moved onto the quality control, using the present .fastq  files instead of the FAST5 files.




## Classification
### Kraken 2

**04-03-2025**

```
conda install bioconda::kraken2
```

Kraken 2 is a tool used for taxonomic classification in metagenomics and was released in 2018 as a more efficient version of Kraken 1 (using less RAM). The tool uses a special kraken classification algorithm with k-mer based classification. This tool is useful for our research, as it can be used to analyse 16S data. A con about kraken 2 is that it has a small false-positive rate (<1%) where it wrongly assigns a read to a species. Another classification tool, KrakenUniq, does not have this false-positive rate @Different_classification_tools. However, as kraken 2 still remains much more efficient we decided to use this tool, as we do not think the small false-positive rate will have a large effect on the outcome.

Kraken assigns taxonomic labels to DNA sequences by examining the k-mers within a query sequence and then query a database. k-mers are substrings within the DNA sequence of length *k*. After quering the database, the k-mers get mapped to the lowest common ancestor (LCA) of all genomes that contain the given k-mer @Kraken2.

We have chosen to use Kraken2, as it seems to be the most efficient and reliable tool available for classification. Another common used classification tool is QIIME 2, however 

The user also can choose the value k that determines the length of the sequences that Kraken uses for its index; every sequence (or k-mer) of length k is associated with the species in which it occurs. K-mers that occur in two or more species are associated with the lowest common ancestor of those species. The database files contain the taxonomy and k-mer information for the specified database. Following generation of these files, Bracken requires users to generate a k-mer distribution file. Kraken and Bracken additionally allow the use of multiple threads to accelerate database construction. 

Kraken2 cannot take more than one file as input, so we will merge all fastq files that belong to the same sample locations. This can be done using:

```
cat FAX*.fastq.gz > sample_1.fastq.gz
```

**06-03-2025**

Kraken2 offers support for 3 public 16S databases @Kraken2:

- Greengenes
- RDP
- SILVA

From these 3 databases, only Greengenes contains all available 16S data, whereas RDP and SILVA use smaller subsets of the data (e.g. containing only bacterial or achaeal 16S data). However RDP and SILVA seem to have deeper taxonomic levels whereas greengenes is more broad (see table). In #Comparing databases I have compared all databases, to see which one classifies the highest amounts of reads, which turned out to be:

We have decided to continue with Greengenes, as we want to make a complete overview of the microbiome in the different steps of the water purification process, so it would make more sense to use a complete database for this and not a subset. We will however test all databases, to make sure that Greengenes is the most thorough database to use in this situation. Possibly RDP or SILVA might be able to classify on a deeper taxonomy level than Greengenes. 

There are also NCBI databases, which include whole genome data of organisms other than micro-organisms (e.g. eukaryota). I have tried to use the largest NCBI database (`core_nt`) and it was able to classify roots outside of the micro-biological roots, however the general results were much less thorough than when using Greengenes or any other 16S database. I imagine this is because `core_nt` is not specialised for classification of 16S data.

Kraken 2 needs the database to have 3 files present, which are

- `taxo.k2d`
- `opts.k2d`
- `hash.k2d`

Normally, one would have to "build" the database using specific Kraken2 commands, however all 16S databases are already present on the server.

In order to use Kraken2 with one of the databases, the database has to be defined with the `--db` flag, followed by the location of the database.

Later we will use Bracken to improve the quality of the Kraken2 output, however, Bracken only takes Kraken2 report files and not the direct output. So we will have to specify the `--report` flag, followed by an output location/name.

Two other flags have to be specified, which are `--gzip-compressed` and `--fastq-input`, to let the tool know how the input file is formatted (file.fastq.gz)

```
kraken2 --db path/to/database path/to/input.fastq.gz --gzip-compressed --fastq-input --report path/to/report.txt > path/to/output/sample_1.kraken
```

(show bit of example output from kraken + what it means)
(show bit of example output from report + what it means)


### Bracken (Bayesian Re-estimation of Abundance with KrakEN)

**11-03-2025**

```
conda install bioconda::bracken
```

Using Bracken after Kraken2 can improve the Kraken2 output. Kraken2 output is often at the highest taxonomic level, using Bracken this can be corrected a deeper taxonomic level. A preferred level in metagenomics is either genus or species (G/S). Bracken uses a statistical method that computes to which e.g. species labels a read should be assigned, based on the number of other reads found within all other species. Combining Kraken2 and Bracken gives a more accurate output on a deeper level taxonomy @Schmeier.

In total 4 arguments have to be given to use Bracken in this scenario:

* `-d`: path to the database
* `-i`: path to the input, which has to be the report output obtained with Kraken2
* `-o`: user-chosen path for where the output will be saved
* `-l`: allows user to choose the taxonomy level that the reads will be assigned as, e.g. S is species and G is genus

For -l we will use genus (`G`), as you will get an error if the chosen rank is lower than the reported taxa from the Kraken 2 output. The lowest possible taxa is Species (`S`), however these are not reported in the Kraken 2 output, while genus are. The error being (https://github.com/jenniferlu717/Bracken/issues/117):

```
Error: no reads found. Please check your Kraken report
```

Bracken can be run using the code below:
```
bracken -d /path/to/database -i /path/to/input/sample_1.report.txt -o /path/to/output.txt -l G

```

The output shows how many reads are newly assigned and to which species, the report output is not used other than for inspection by the user. Another file automatically gets added in the same directory as the report (`{sample}_bracken_genuses.txt`), containing the original report that is corrected with the output from Bracken. This is the file that we will use further down the pipeline. 


```
rule kraken2_taxonomic_classification:
    input:
        reads="trimmed/{sample}.fastq"
    output:
        report="reports/kraken2/{sample}_report.txt",
        output="kraken2/{sample}_output.txt"
    log:
        "logs/kraken2/{sample}.log"
    params:
        db="/data/datasets/KRAKEN2_INDEX/16S_Greengenes/",  
        confidence="0.1",               
        threads=128                    
    shell:
        """
        kraken2 --db {params.db} \
                --threads {params.threads} \
                --confidence {params.confidence} \
                --output {output.output} \
                --report {output.report} \
                --gzip-compressed
                {input.reads} 2> {log}
        """
        
rule bracken_estimation:
    input:
        kraken_report="reports/kraken2/{sample}_report.txt"
    output:
        
    log:
      "logs/bracken/{sample}.log"
    shell:
    """
    bracken -d {params.db} \
    -i {input} \
    -o {output} \
    -l G

    """
```

## Functional analysis

**21-03-2025**

Now that we have gotten a general idea of what kind of micro organisms are present in the lagoon influent and effluent, we can take a closer look at what kind of processes these organisms indulge in. We can do this by doing a pathway analysis of all involved Operational Taxonomic Units (OTU's), this can refer to any classification such as a species, genus or class. But in this case we are only looking at species, as Bracken has been used to further classify any Kraken2 classifications to species level.

We can use the output of Kraken2 in a tool called `FAPROTAX`, this is a database that maps prokaryotic OTU's against metabolic- and other ecological functions @Louca. Examples include processes such as methanogenesis, fermentation and denitrification. However Kraken2 output can not directly be used, first the output has to be converted to .BIOM format. This can be done using the kraken-biom tool.

### kraken-biom

**21-03-2025**

Using kraken-biom, the output of Kraken2 can be converted to .BIOM format, which is the input needed for FAPROTAX. 

Installing kraken-biom, using bioconda:
```
conda install bioconda::kraken-biom
```

Converting Kraken2 output to .BIOM format:

```
kraken-biom /path/to/input.kraken -o /path/to/output.biom --fmt json

```

Use --fmt json to specify that the biom output should have a json format (V.1.0). I did this, because the example .biom input in the faprotax documentation also uses json formatting. Kraken-biom can also output to a newer .biom format (V.2.x), which is HDF5 (Hierarchical Data Format version 5) format. But I decided to settle for the older .biom format as that seems to work well.


### FAPROTAX (Functional Annotation of Prokaryotic Taxa)

**23-03-2025**

FAPROTAX uses a database of prokaryotic metabolic pathways (FAPROTAX.txt), which is essentially an extensive list describing different pathways, e.g. 'arsenate_detoxification', followed up by names of different prokaryotic species that partake in this pathway and including a reference to a paper which states the former. Below you can see an example of a snippet of this database.

```
arsenate_detoxification		

elements:As; main_element:As; electron_donor:variable; electron_acceptor:As; aerobic:variable;
exclusively_prokaryotic:yes; light_dependent:no

# dissimilatory reduction of arsenate for detoxification
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
*Sinorhizobium*M14*									      # DOI:10.1080/01490450802402836
*Bacillus*SXB*										        # DOI:10.1111/jam.12093
*Pantoea*IMH*										          # DOI:10.1111/jam.12093
*Microbacterium*hydrocarbonoxydans*				# DOI:10.1111/j.1365-2672.2009.04188.x
*Variovorax*paradoxus*								    # DOI:10.1111/j.1365-2672.2009.04188.x
*Kocuria*erythromyxa*								      # DOI:10.1111/j.1365-2672.2009.04188.x
*Arsenicoccus*bolidensis
```

Faprotax does not have a github repo, so everything has to be downloaded from their website (http://www.loucalab.com/archive/FAPROTAX/lib/php/index.php?section=Download). I downloaded the latest version (V.1.2.10) which is a zipped file that contains the faprotax database (`faprotax.txt`), a README and a python script (`collapse_table`).

I used the following arguments:

- i: input file (has to be json formatted .biom, containing info on species + abundances)
- o: output file, shows all pathways included in the faprotax.txt database, with a corresponding number displaying how active this pathway is in the entire sample (or 'functional group abundances per sample'). See the example below:

```
sulfate_respiration	0.006833712984
sulfur_respiration	0.005150044568
dark_sulfite_oxidation	0
sulfite_respiration	0.004159651382
thiosulfate_respiration	0
respiration_of_sulfur_compounds	0.01198375755
arsenate_detoxification	0

```

-g path to the faprotax database (faprotax.txt)
-n choose whether the output table should be normalized or not, by normalizing it will convert the number of OTU's to ratios. There are multiple different options:

- `none` (default)
- `columns_before_collapsing`
- `columns_after_collapsing` 
- `columns_before_collapsing_excluding_unassigned`

I chose for columns_after_collapsing, because it gives a good overview of the ratio of metabolic processes in the sample, as it only counts the OTU's with assigned functions and ignores the unassigned OTU's. After using this form of normalization, all active metabolic processes together add up to 1. The higher the number, the more represented the metabolic process is in this sample, so the more OTU's there are that perform this metabolic process.


before normalizing: 

```
group	in_lagoon_report_bracken_genuses
sulfate_respiration	138
sulfur_respiration	104
```

after normalizing:

```
group	in_lagoon_report_bracken_genuses
sulfate_respiration	0.006833712984
sulfur_respiration	0.005150044568
```

- `--collapse_by_metadata`
Specify what OTU metadata field contains the taxonomic paths in the .biom input. kraken-biom sets this to `taxonomy`.

```
python3 collapse_table.py \
-i /path/to/input.biom \
-o /path/to/output.tsv \
-g FAPROTAX.txt \
-n columns_after_collapsing \
--collapse_by_metadata "taxonomy"\
--out_sub_tables_dir /path/to/output/dir
--out_groups2records_table /path/to/output/table.txt \
--out_report /path/to/output/.txt \ 
--force \
-v \

```
### Minor adjustments faprotax script
**25-03-2025**
faprotax used the python script `collapse_table.py` to assign OTU's to corresponding (metabolic) pathways. However at first when I tried to run this script with the bash command, I would get errors coming from the script, mentioning unescaped escape characters. All I had to do to fix this was to change the `\` in the script to `\\`.



## Visualisation

### Krona

**07-03-2025**

Krona is one of the tools we will use to visualise the data obtained through the other tools. Krona visualises classification data in a multi-layered pie chart. These charts are interactive and can be viewed within a webbrowser @Krona. The visualisation that follows from using this tool could prove as a nice overview of the classification, for in our paper.

Below I describe how to use Krona, using Kraken2 output @AndreaTelatin, @Schmeier:

```
conda install bioconda::krona
```
After installing Krona, an installation message will show that the taxonomy databases need to manually be updated before Krona can generate taxonomic reports. For this, the following script has to be ran:

```
ktUpdateTaxonomy.sh
```

update taxonomy naar conda locatie heel belangrijk!!! en in envs overal python versie zetten yay

ktUpdateTaxonomy.sh /students/2024-2025/Thema07/metagenomics/wastewater/.snakemake/conda/692460f6e8eec96f3093b62149341dd8_/opt/krona/taxonomy
Fetching taxdump.tar.gz...

Krona can be run using the output of Kraken2, the two relevant variables are the counts (-m) and the NCBI taxonomy ID (-t). These variables are found in Kraken2 reports, in column 3 and 5.

In the bash command you have to let Krona know which columns to use, and Krona can be run using:

```
ktImportTaxonomy -t 5 -m 3 {input} -o {output}
```


**16-03-2025**

I have ran all the above described steps and generated Krona plots for all of them, however lagoon in and lagoon out seem to contain large amounts of genetic data that is not identified by Kraken2, with the Greengenes database. 39% of the data of `Lagoon out` is assigned to "other root", and 16% of `Lagoon in`. A potential fix for this, could be to switch to a different, larger database. One that contains more than just 16S data of micro organisms. Although our focus for this research is on micro-organisms, the percentages of genetic data that fall outside of micro-organisms seems relatively large and could be interesting to look further into.

Another issue we have run into is that the data for the digestor does not seem sufficient. Earlier I described how sample 10 and 12 were fully absent from the Minion output, however we expected to be able to work with the remaining data from sample 9 and 11. In total, only 2 species are identified from the two samples, which does not seem representative of the sample area it was taken from. As I expect many (anaerobic) bacteria to be present in the digester (verwijzen naar artikel over metagenomics analyze van zo'n soort digester).


# Snakemake

Floris has written the SnakeMake code for fastplong, bracken and Kraken2. I have made the Snakemake code for kraken-biom, faprotax and krona:

```
rule all:
    input:

        expand("krona/{sample}_krona.html", sample=config["samples"].keys()),
        expand("logs/krona/{sample}.log", sample=config["samples"].keys()), 
    
rule krona_visualisation:
    input:
        bracken_files="bracken/reports/{sample}.txt"
    output: 
        krona_vis="krona/{sample}_krona.html"
    log:
        "logs/krona/{sample}.log"
    conda:
        "envs/krona.yaml"
    shell:
        """
        ktImportTaxonomy -t 5 \
                         -m 3 \
                         {input} \
                         -o {output} \
                         2> {log}
        """
```


```
rule all:
    input:
    
        expand("kraken_biom/{sample}_output.biom", sample=config["samples"].keys()),
        expand("logs/kraken_biom/{sample}.log", sample=config["samples"].keys()),

rule kraken_biom:
    input:  
        bracken_kraken="bracken/reports/{sample}.txt"		
    output: 
        biom_files="kraken_biom/{sample}_output.biom"
    log:
        "logs/kraken_biom/{sample}.log"
    conda:
        "envs/kraken_biom.yaml"
    shell:
        """
        kraken-biom {input} \
                    -o {output} \
                    --fmt json \
                    2> {log}
        """
```
```
rule all:
    input:
    
        expand("faprotax/{sample}/{sample}_function_table.tsv", sample=config["samples"].keys()),
        expand("faprotax/{sample}/{sample}_report.txt", sample=config["samples"].keys()),
        expand("faprotax/{sample}/group_table.txt", sample=config["samples"].keys()),
        expand("faprotax/{sample}/sub_groups/", sample=config["samples"].keys()),
        expand("logs/faprotax/{sample}.log", sample=config["samples"].keys())

rule faprotax:
    input:
        biom_files="kraken_biom/{sample}_output.biom"
    output:
        function_table="faprotax/{sample}/{sample}_function_table.tsv",
        faprotax_report="faprotax/{sample}/{sample}_report.txt",
        sub_table_dir=directory("faprotax/{sample}/sub_groups/"),
        groups_to_records="faprotax/{sample}/group_table.txt"
    log:
        "logs/faprotax/{sample}.log"
    shell:
        """
        python3 tools/FAPROTAX_1.2.10/collapse_table.py \
        -i {input.biom_files} \
        -o {output.function_table} \
        -g tools/FAPROTAX_1.2.10/FAPROTAX.txt \
        -n columns_after_collapsing \
        --collapse_by_metadata "taxonomy" \
        --out_sub_tables_dir {output.sub_table_dir} \
        --out_groups2records_table {output.groups_to_records} \
        --out_report {output.faprotax_report} \
        -v \
        2> {log}
        """
```

## Building envs for the tools

**26-03-2025**
To make it easier for other users to use this pipeline, without them having to download/install all the used tools/packages I will make environments for all tools that are available through bioconda. For this I use the following format, in a `.yaml` file:

```
name: kraken_biom

channels:
  - bioconda
  
dependencies:
  - kraken-biom = 1.2.0
  - python = 3.12.8
```

For the dependencies I enter the used tool, including the version that we have made the pipeline with and I specify the python version the pipeline was made with. At first I had not specified the python version. which resulted in me getting an error where conda was trying to build the kraken-biom environment using an old version of python (2.7), but after specifying the correct version of python this problem was solved.


# Comparing databases
To test which 16S database gives the highest percentage of classified sequences, I will run the kraken2-bracken-krona pipeline for `in_lagoon.fastq.gz` for all 3 databases (Greengenes, RDP, Silva). I decided to test it on this file, because `in_lagoon.fastq.gz` contains the most reads out of the 3 different sample locations for this project. `in_lagoon.fastq.gz` contains a gz compressed fastq file in which samples 1-4 have been merged with cat, because Kraken2 only takes 1 input file. `in_lagoon.fastq.gz` has also been quality checked and trimmed accordingly using fastplong. These 2 pre-processing steps have been performed by Floris and more information about this can be found in his logbook.

### Greengenes
### RDP

While running Krona, I got the following error:


```
[ WARNING ]  The following taxonomy IDs were not found in the local database and were set to root (if they were recently added to NCBI, use updateTaxonomy.sh to update the local database): 
653 2863 2963 36 790 175 514 30 1093 698 2083 877 2837 417 677 3121 390 3 388 874 2545 336 871 591 2859 2954 682 345 2847 2860 2883 369 457 2967
```

Presumably these are sequences from non-microbiological organisms, which are not present in the 16S databases.


Less not found sequences after correcting to genus with bracken


Kraken2 output for in_lagoon:
```
20242 sequences (28.87 Mbp) processed in 4.575s (265.5 Kseq/m, 378.64 Mbp/m).
  20215 sequences classified (99.87%)
  27 sequences unclassified (0.13%)
```
bracken results

```
>> Checking for Valid Options...
 >> Running Bracken 
      >> python src/est_abundance.py -i /students/2024-2025/Thema07/metagenomics/wastewater/data/kraken_output_rdp/in_lagoon_report.txt -o /students/2024-2025/Thema07/metagenomics/wastewater/data/bracken_output_rdp/in_lagoon.bracken -k /data/datasets/KRAKEN2_INDEX/16S_RDP/database100mers.kmer_distrib -l G -t 10
PROGRAM START TIME: 03-24-2025 12:14:53
>> Checking report file: /students/2024-2025/Thema07/metagenomics/wastewater/data/kraken_output_rdp/in_lagoon_report.txt
BRACKEN SUMMARY (Kraken report: /students/2024-2025/Thema07/metagenomics/wastewater/data/kraken_output_rdp/in_lagoon_report.txt)
    >>> Threshold: 10 
    >>> Number of genuses in sample: 210 
          >> Number of genuses with reads > threshold: 62 
          >> Number of genuses with reads < threshold: 148 
    >>> Total reads in sample: 20242
          >> Total reads kept at genuses level (reads > threshold): 12868
          >> Total reads discarded (genuses reads < threshold): 344
          >> Reads distributed: 528
          >> Reads not distributed (eg. no genuses above threshold): 6475
          >> Unclassified reads: 27
BRACKEN OUTPUT PRODUCED: /students/2024-2025/Thema07/metagenomics/wastewater/data/bracken_output_rdp/in_lagoon.bracken
PROGRAM END TIME: 03-24-2025 12:14:53
  Bracken complete.
```

### Silva



                
# Krona results
### Digestor
![](img/digester_krona.svg)


### Lagoon in
![](img/lagoon_in_krona.svg)


### Lagoon out

![](img/lagoon_out_krona.svg)
(per figuur nog wat zeggen)


# Conclusion

- answer hypothesis/conclusion



